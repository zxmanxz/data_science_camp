{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data \n",
    "\n",
    "</font>\n",
    "\n",
    "[Sentiment Analysis Dataset](https://www.kaggle.com/sonaam1234/sentimentdata)\n",
    "\n",
    "alternative source: \n",
    "<br>\n",
    "[rt-polaritydata](https://github.com/dennybritz/cnn-text-classification-tf/tree/master/data/rt-polaritydata)\n",
    "\n",
    "alternative source: \n",
    "<br>\n",
    "[Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data)\n",
    "\n",
    "Each line in these two files corresponds to a single snippet (usually containing roughly one single sentence); all snippets are down-cased.  \n",
    "[More info about dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " simplistic , silly and tedious . \n",
      "\n",
      " it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      " exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      " [garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      " a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n"
     ]
    }
   ],
   "source": [
    "fn = os.path.join(path, 'data/rt-polarity.neg')\n",
    "\n",
    "with open(fn, \"r\",encoding='latin-1') as f:\n",
    "    content = f.read()  \n",
    "texts_neg = np.array(content.splitlines())\n",
    "\n",
    "for review in texts_neg[:5]:\n",
    "    print ('\\n', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      " the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      " effective but too-tepid biopic\n",
      "\n",
      " if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      " emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n"
     ]
    }
   ],
   "source": [
    "fn = os.path.join(path, 'data/rt-polarity.pos')\n",
    "\n",
    "with open(fn, \"r\",encoding='latin-1') as f:\n",
    "    content = f.read()\n",
    "texts_pos = np.array(content.splitlines())\n",
    "\n",
    "for review in texts_pos[:5]:\n",
    "    print ('\\n', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_neg = texts_neg.reshape(-1,1)\n",
    "texts_neg = np.append(texts_neg,np.zeros(texts_neg.shape),axis=1)\n",
    "\n",
    "texts_pos = texts_pos.reshape(-1,1)\n",
    "texts_pos = np.append(texts_pos,np.ones(texts_pos.shape),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['simplistic , silly and tedious . ', '0.0'],\n",
       "       [\"it's so laddish and juvenile , only teenage boys could possibly find it funny . \",\n",
       "        '0.0'],\n",
       "       ['exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . ',\n",
       "        '0.0'],\n",
       "       ...,\n",
       "       ['standing in the shadows of motown is the best kind of documentary , one that makes a depleted yesterday feel very much like a brand-new tomorrow . ',\n",
       "        '1.0'],\n",
       "       [\"it's nice to see piscopo again after all these years , and chaykin and headly are priceless . \",\n",
       "        '1.0'],\n",
       "       ['provides a porthole into that noble , trembling incoherence that defines us all . ',\n",
       "        '1.0']], dtype='<U269')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.append(texts_neg, texts_pos, axis = 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[:,0], data[:,1].astype(np.float).astype(np.int16), \n",
    "                                                    random_state = 2019 )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100077"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf_vectorizer= TfidfVectorizer().fit(X_train)\n",
    "# X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C = 3, tol = 0.06, max_iter = 200, random_state = 2019).fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.767\n",
      "AUC: 0.839 \n",
      "Accuracy: 0.769\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test_vectorized) # Predict the transformed test documents\n",
    "print('f1: %0.03f' %f1_score(y_test, predictions))\n",
    "scores = clf.decision_function(X_test_vectorized) \n",
    "print('AUC: %0.03f ' %roc_auc_score(y_test, scores)) \n",
    "print('Accuracy: %0.03f' %clf.score(X_test_vectorized,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest coefs:\n",
      "['dull' 'routine' 'boring' 'pretentious' 'flat' 'too' 'unfunny' 'worst'\n",
      " 'mediocre' 'tedious']\n",
      "\n",
      "Largest coefs: \n",
      "['wonderful' 'solid' 'fun' 'treat' 'still' 'enjoyable' 'masterpiece'\n",
      " 'cinema' 'smart' 'engrossing']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "print('Smallest coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
